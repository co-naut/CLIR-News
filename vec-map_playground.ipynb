{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7413c2e",
   "metadata": {},
   "source": [
    "Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3cddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_mapping import EmbeddingLoader\n",
    "\n",
    "EMBEDDING_PATH_ROOT = \"./pretrained_word2vec/\"\n",
    "EN_EMBEDDING = \"en/GoogleNews-vectors-negative300.bin.gz\"\n",
    "ZH_EMBEDDING = \"zh/sgns.merge.word.bz2\"\n",
    "\n",
    "MAX_VOCAB = 10000\n",
    "\n",
    "loader = EmbeddingLoader()\n",
    "\n",
    "en_emb = loader.load_word2vec(filepath=EMBEDDING_PATH_ROOT+EN_EMBEDDING, max_vocab=MAX_VOCAB)\n",
    "zh_emb = loader.load_word2vec(filepath=EMBEDDING_PATH_ROOT+ZH_EMBEDDING, max_vocab=MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vk5ds9g8lw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_embedding(word, embeddings):\n",
    "    if word not in embeddings:\n",
    "        raise KeyError(f\"Word '{word}' not found in embeddings\")\n",
    "    return embeddings[word]\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Calculate magnitudes (L2 norms)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d1dbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11028192\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"一\", zh_emb), get_word_embedding(\"one\", en_emb))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62160185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684622\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"一\", zh_emb), get_word_embedding(\"一个\", zh_emb))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e156f422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4903487\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"one\", en_emb), get_word_embedding(\"single\", en_emb))\n",
    "\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
