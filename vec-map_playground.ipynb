{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7413c2e",
   "metadata": {},
   "source": [
    "## EmbeddingLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3cddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary: 10000 words\n",
      "Chinese vocabulary: 10000 words\n"
     ]
    }
   ],
   "source": [
    "from vector_mapping import EmbeddingLoader\n",
    "\n",
    "EMBEDDING_PATH_ROOT = \"./pretrained_word2vec/\"\n",
    "EN_EMBEDDING = \"en/GoogleNews-vectors-negative300.bin.gz\"\n",
    "ZH_EMBEDDING = \"zh/sgns.merge.word.bz2\"\n",
    "\n",
    "MAX_VOCAB = 10000\n",
    "\n",
    "# Create separate loaders for each language (new stateful API requires separate instances)\n",
    "en_loader = EmbeddingLoader()\n",
    "zh_loader = EmbeddingLoader()\n",
    "\n",
    "# Load embeddings\n",
    "en_loader.load_word2vec(filepath=EMBEDDING_PATH_ROOT+EN_EMBEDDING, max_vocab=MAX_VOCAB)\n",
    "zh_loader.load_word2vec(filepath=EMBEDDING_PATH_ROOT+ZH_EMBEDDING, max_vocab=MAX_VOCAB)\n",
    "\n",
    "# Get embeddings dicts for use with helper functions below\n",
    "en_emb = en_loader.get_embeddings()\n",
    "zh_emb = zh_loader.get_embeddings()\n",
    "\n",
    "# Show vocabulary sizes\n",
    "print(f\"English vocabulary: {len(en_loader)} words\")\n",
    "print(f\"Chinese vocabulary: {len(zh_loader)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vk5ds9g8lw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_embedding(word, embeddings):\n",
    "    if word not in embeddings:\n",
    "        raise KeyError(f\"Word '{word}' not found in embeddings\")\n",
    "    return embeddings[word]\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Calculate magnitudes (L2 norms)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d1dbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11028192\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"一\", zh_emb), get_word_embedding(\"one\", en_emb))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62160185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684622\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"一\", zh_emb), get_word_embedding(\"一个\", zh_emb))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e156f422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4903487\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"one\", en_emb), get_word_embedding(\"single\", en_emb))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fk6suwa457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word existence checks:\n",
      "  'cat' in en_loader: True\n",
      "  'dog' in en_loader: True\n",
      "  'asdfghjkl' in en_loader: False\n",
      "\n",
      "Get single word embedding:\n",
      "  'cat' embedding shape: (300,)\n",
      "  'cat' embedding (first 10 dims): [ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
      "  0.04980469 -0.00952148  0.22070312 -0.12597656]\n",
      "\n",
      "Vocabulary operations:\n",
      "  English vocab sample: ['misconduct', 'ancient', 'fluid', 'speeding', 'Huskies', 'Iraqis', 'road', 'disappeared', 'hearing', 'announce']\n",
      "  Chinese vocab sample: ['想到', '旋律', '2009', '手动', '兑现', '恶化', '火电', '鼓励', '加深', '保']\n",
      "\n",
      "Method chaining example:\n",
      "  Filtered vocab size: 2\n",
      "  Filtered words: {'house', 'car'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Word existence checks:\")\n",
    "print(f\"  'cat' in en_loader: {'cat' in en_loader}\")\n",
    "print(f\"  'dog' in en_loader: {'dog' in en_loader}\")\n",
    "print(f\"  'asdfghjkl' in en_loader: {'asdfghjkl' in en_loader}\")\n",
    "print()\n",
    "\n",
    "print(\"Get single word embedding:\")\n",
    "cat_vec = en_loader.get_embedding('cat')\n",
    "print(f\"  'cat' embedding shape: {cat_vec.shape}\")\n",
    "print(f\"  'cat' embedding (first 10 dims): {cat_vec[:10]}\")\n",
    "print()\n",
    "\n",
    "print(\"Vocabulary operations:\")\n",
    "en_vocab = en_loader.get_vocabulary()\n",
    "zh_vocab = zh_loader.get_vocabulary()\n",
    "print(f\"  English vocab sample: {list(en_vocab)[:10]}\")\n",
    "print(f\"  Chinese vocab sample: {list(zh_vocab)[:10]}\")\n",
    "print()\n",
    "\n",
    "print(\"Method chaining example:\")\n",
    "test_loader = EmbeddingLoader()\n",
    "test_loader.load_word2vec(\n",
    "    filepath=EMBEDDING_PATH_ROOT+EN_EMBEDDING, \n",
    "    max_vocab=1000\n",
    ").filter_vocabulary({'cat', 'dog', 'house', 'tree', 'car'})\n",
    "print(f\"  Filtered vocab size: {len(test_loader)}\")\n",
    "print(f\"  Filtered words: {test_loader.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0ob3gl0w2j",
   "metadata": {},
   "source": [
    "## DictionaryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ppsbhry56q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dictionary pairs: 21597\n",
      "Filtered pairs (words with embeddings): 5345\n",
      "Sample pairs: [('年', 'year'), ('月', 'moon'), ('月', 'months'), ('月', 'month'), ('日', 'day'), ('村', 'village'), ('人', 'man'), ('人', 'people'), ('%', '%'), ('大', 'big')]\n"
     ]
    }
   ],
   "source": [
    "from vector_mapping import DictionaryParser\n",
    "\n",
    "# Parse MUSE dictionary with only simplified Chinese\n",
    "parser = DictionaryParser()\n",
    "parser.parse_muse_format('./dictionaries/muse-zh-en.txt', include_traditional=False)\n",
    "print(f\"Total dictionary pairs: {len(parser.get_pairs())}\")\n",
    "\n",
    "# Filter by available embeddings\n",
    "parser.filter_by_vocabulary(zh_vocab, en_vocab)\n",
    "print(f\"Filtered pairs (words with embeddings): {len(parser.get_pairs())}\")\n",
    "\n",
    "# View sample pairs\n",
    "print(f\"Sample pairs: {parser.get_pairs()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7y4364ix9t",
   "metadata": {},
   "source": [
    "## Cross-Lingual Translation Pair Similarities (Unmapped Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0zw0p7itqlqe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10 translation pairs from dictionary:\n",
      "\n",
      "年        <-> year                 : -0.0347\n",
      "月        <-> moon                 : 0.0117\n",
      "月        <-> months               : -0.0639\n",
      "月        <-> month                : 0.0526\n",
      "日        <-> day                  : 0.0688\n",
      "村        <-> village              : -0.0393\n",
      "人        <-> man                  : -0.0390\n",
      "人        <-> people               : 0.0672\n",
      "%        <-> %                    : -0.0239\n",
      "大        <-> big                  : -0.0061\n",
      "\n",
      "Results:\n",
      "  Pairs evaluated: 10/10\n",
      "  Average similarity: -0.0007\n",
      "  Std deviation: 0.0459\n",
      "  Min similarity: -0.0639\n",
      "  Max similarity: 0.0688\n"
     ]
    }
   ],
   "source": [
    "# Use translation pairs from the loaded dictionary parser\n",
    "pairs = parser.get_pairs()\n",
    "\n",
    "# Sample a subset if there are too many pairs (optional)\n",
    "sample_size = min(10, len(pairs))\n",
    "sampled_pairs = pairs[:sample_size]\n",
    "\n",
    "print(f\"Evaluating {sample_size} translation pairs from dictionary:\\n\")\n",
    "\n",
    "similarities = []\n",
    "found_count = 0\n",
    "\n",
    "for zh_word, en_word in sampled_pairs:\n",
    "    try:\n",
    "        # Get embeddings for both words\n",
    "        zh_vec = get_word_embedding(zh_word, zh_emb)\n",
    "        en_vec = get_word_embedding(en_word, en_emb)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim = cosine_similarity(zh_vec, en_vec)\n",
    "        similarities.append(sim)\n",
    "        found_count += 1\n",
    "\n",
    "        print(f\"{zh_word:8s} <-> {en_word:20s} : {sim:.4f}\")\n",
    "    except KeyError as e:\n",
    "        # Skip pairs where word not in embeddings\n",
    "        continue\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Pairs evaluated: {found_count}/{sample_size}\")\n",
    "print(f\"  Average similarity: {np.mean(similarities):.4f}\")\n",
    "print(f\"  Std deviation: {np.std(similarities):.4f}\")\n",
    "print(f\"  Min similarity: {np.min(similarities):.4f}\")\n",
    "print(f\"  Max similarity: {np.max(similarities):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
